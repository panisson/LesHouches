{
 "metadata": {
  "name": "",
  "signature": "sha256:afa678090e2aff62d2a89104f078fdf4a3a9c39cfd1184bc82e08316de6730c2"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Complex Networks Thematic School, Les Houches, April 7-18, 2014\n",
      "\n",
      "---\n",
      "\n",
      "# Machine Learning - Examples\n",
      "\n",
      "##Dr. Andr\u00e9 Panisson\n",
      "\n",
      "---"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 1. Hand-written digits recognizer"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The MNIST dataset contains 70000 images of hand-written digits. Each image is a matrix of 28x28 grayscale pixels. The value of each pixel goes from 0 to 255.\n",
      "\n",
      "There is more information available in the MNIST dataset repository:\n",
      "http://yann.lecun.com/exdb/mnist/\n",
      "\n",
      "Let's first fetch the dataset from the internet (which may take a while, note the asterisk [*]):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import fetch_mldata\n",
      "X, _, _, y = fetch_mldata(\"MNIST Original\", data_home='.').values() # fetch dataset from internet\n",
      "print X.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This dataset is ordered, and we will get just part of it to speed up this exercise.\n",
      "\n",
      "For this, we need to shuffle the entire data and get the first instances."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.utils import shuffle\n",
      "\n",
      "X, y = shuffle(X, y) # shuffle dataset (which is ordered!)\n",
      "X = X[:1000]        # take only the first instances, to shorten runtime\n",
      "y = y[:1000]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "From the sklearn documentation:\n",
      "\n",
      "> Many elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the l1 and l2 regularizers of linear models) assume that all features are centered around zero and have variance in the same order. If a feature has a variance that is orders of magnitude larger that others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected.\n",
      "\n",
      "We will then rescale the training data, since this is important to improve the performance of the classification algorithm that we will use later."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Rescale grayscale from -1 to 1\n",
      "X = X/255.0*2 - 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's have a look at some of the instances in the dataset we just loaded:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.rc(\"image\", cmap=\"binary\")\n",
      "plt.figure(figsize=(8,4))\n",
      "for i in xrange(10):\n",
      "    plt.subplot(2,5,i+1)\n",
      "    plt.imshow(X[i].reshape(28,28))\n",
      "    plt.title(y[i])\n",
      "    plt.xticks(())\n",
      "    plt.yticks(())\n",
      "plt.tight_layout()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "PCA (Principal Component Analysis) converts a dataset of possibly correlated features into a dataset of linearly uncorrelated features called principal components. \n",
      "It is used to decompose a multivariate dataset in a set of successive orthogonal components that explain a maximum amount of the variance.\n",
      "\n",
      "The optional parameter `whiten=True` make it possible to project the data onto the singular space while scaling each component to unit variance. This is often useful if the models down-stream make strong assumptions on the isotropy of the signal: this is for example the case for Support Vector Machines with the RBF kernel and the K-Means clustering algorithm."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "By setting the parameter `n_components` to $\\mathbf{k}$, we limit the number of components to $\\mathbf{k}$. \n",
      "As consequence, the value returned from `transform` is in fact a matrix $\\mathbf{U} \\in \\mathbb{R}^{n \\times k}$ with the $\\mathbf{k}$ most representative components of the dataset."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.decomposition import PCA\n",
      "pca = PCA(50)\n",
      "pca.fit(X)\n",
      "U = pca.transform(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "From now on, we will try to train a SVM classificator to classify the numbers.\n",
      "\n",
      "For this, we create a SVM classifier with **rbf** kernel and default parameters and assign it to a variable with name **`svm`**.\n",
      "Then, we create a list named **`gammas`** with 10 values for the rbf coefficient (gamma) starting from $10^{-4}$ to $10^{0}$, spaced evenly on a log scale. For each value of gamma, we set the model parameter **`gamma`** to this value, calculate a list of 3-fold cross-validation scores, and add the average score to a list with name **`scores`**. We use **`U`** and **`y`** to train the model.\n",
      "\n",
      "Finally, we plot the score for each gamma value."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import svm, cross_validation\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "pca = PCA(n_components=30)\n",
      "U = pca.fit_transform(X)\n",
      "\n",
      "classifier = svm.SVC(kernel='rbf')\n",
      "gammas = np.logspace(-4, 0, 10)\n",
      "scores = []\n",
      "kfold = cross_validation.KFold(len(y), n_folds=3,\n",
      "                               shuffle=True, random_state=0)\n",
      "\n",
      "for gamma in gammas:\n",
      "    classifier.gamma = gamma\n",
      "    this_scores = cross_validation.cross_val_score(classifier,\n",
      "                                                   U, y, cv=kfold)\n",
      "    scores.append(np.average(this_scores))\n",
      "    \n",
      "print np.max(scores)\n",
      "semilogx(gammas, scores)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's optimize both parameters **gamma** and **C** by using the Grid Search."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.grid_search import GridSearchCV\n",
      "\n",
      "from sklearn import svm, cross_validation\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "pca = PCA(n_components=30)\n",
      "U = pca.fit_transform(X)\n",
      "\n",
      "classifier = svm.SVC(kernel='rbf')\n",
      "gammas = np.logspace(-3, -1, 6)\n",
      "Cs = np.logspace(-2, 2, 6)\n",
      "\n",
      "kfold = cross_validation.KFold(len(y), n_folds=3,\n",
      "                               shuffle=True, random_state=0)\n",
      "\n",
      "gs = GridSearchCV(classifier, {'gamma': gammas, 'C': Cs}, cv=kfold)\n",
      "gs.fit(U, y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gs.best_params_, gs.best_score_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 2. Building a Text Message Spam Filter with a Naive Bayes Classifier\n",
      "\n",
      "This example explains how to classify text messages as spam / not spam using scikit-learn."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Download Dataset\n",
      "The SMS Spam Collection is open source and available at the UCI Machine Learning Repository.\n",
      "The data files and documentation can be found here: http://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection\n",
      "\n",
      "###Reading the Dataset into Memory"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "messages = []\n",
      "categories = []\n",
      "for line in open(\"data/smsdata.txt\"):\n",
      "    category, message = line.split('\\t')\n",
      "    messages.append(message)\n",
      "    categories.append(category)\n",
      "\n",
      "y = array([0 if item==\"ham\" else 1 for item in categories])\n",
      " \n",
      "print\n",
      "print \" %d Not Spam\" % (y==0).sum()\n",
      "print \"+ %d Spam\" % (y==1).sum()\n",
      "print \" ---------\"\n",
      "print \" %d Total\" % len(y)\n",
      "print \n",
      "print \"Proportion spam: %.2f/100\" % (100.*(y==1).sum() / float(len(y)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##From Text Messages to Feature Vectors\n",
      "We need to transform our text data into feature vectors, numerical representations which are suitable for performing statistical analysis. The most common way to do this is to apply a bag-of-words approach where the frequency of an occurrence of a word becomes a feature for our classifier.\n",
      "\n",
      "\n",
      "###Term Frequency-Inverse Document Frequency\n",
      "\n",
      "We want to consider the relative importance of particular words, so we'll use term frequency\u2013inverse document frequency as a weighting factor. This will control for the fact that some words are more \"spamy\" than others.\n",
      "\n",
      "### Mathematical details\n",
      "\n",
      "tf\u2013idf is the product of two statistics, term frequency and inverse document\n",
      "frequency. Various ways for determining the exact values of both statistics\n",
      "exist. In the case of the '''term frequency''' tf(''t'',''d''), the simplest\n",
      "choice is to use the ''raw frequency'' of a term in a document, i.e. the\n",
      "number of times that term ''t'' occurs in document ''d''. If we denote the raw\n",
      "frequency of ''t'' by f(''t'',''d''), then the simple tf scheme is\n",
      "tf(''t'',''d'') = f(''t'',''d''). Other possibilities\n",
      "include:\n",
      "\n",
      "  * boolean_data_type \"frequencies\": tf(''t'',''d'') = 1 if ''t'' occurs in ''d'' and 0 otherwise; \n",
      "  * logarithmically scaled frequency: tf(''t'',''d'') = log (f(''t'',''d'') + 1); \n",
      "  * augmented frequency, to prevent a bias towards longer documents, e.g. raw frequency divided by the maximum raw frequency of any term in the document: $$\\mathrm{tf}(t,d) = 0.5 + \\frac{0.5 \\times \\mathrm{f}(t, d)}{\\max\\{\\mathrm{f}(w, d):w \\in d\\}}$$\n",
      "\n",
      "The '''inverse document frequency''' is a measure of whether the term is\n",
      "common or rare across all documents. It is obtained by dividing the total\n",
      "number of documents by the number of documents containing the\n",
      "term, and then taking the logarithm of that quotient.\n",
      "\n",
      "$$\\mathrm{idf}(t, D) = \\log \\frac{|D|}{|\\{d \\in D: t \\in d\\}|}$$\n",
      "\n",
      "with\n",
      "\n",
      "  * $|D|$: cardinality of D, or the total number of documents in the corpus \n",
      "  * $|\\{d \\in D: t \\in d\\}|$ : number of documents where the term $t$ appears (i.e., $\\mathrm{tf}(t,d) eq 0$). If the term is not in the corpus, this will lead to a division-by-zero. It is therefore common to adjust the formula to $1 + |\\{d \\in D: t \\in d\\}|$. \n",
      "\n",
      "Mathematically the base of the log function does not matter and constitutes a\n",
      "constant multiplicative factor towards the overall result.\n",
      "\n",
      "Then tf\u2013idf is calculated as\n",
      "\n",
      "$$\\mathrm{tfidf}(t,d,D) = \\mathrm{tf}(t,d) \\times \\mathrm{idf}(t, D)$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
      "\n",
      "pattern ='(?u)\\\\b[A-Za-z]{3,}'\n",
      "\n",
      "cv = CountVectorizer(stop_words=None, token_pattern=pattern, ngram_range=(1, 3))\n",
      "C = cv.fit_transform(messages)\n",
      "\n",
      "tfidf = TfidfTransformer(sublinear_tf=True)\n",
      "                        \n",
      "#calculate features using tf-idf and create a training set \n",
      "X_train = tfidf.fit_transform(C)\n",
      "print \n",
      "print \"X_train is a sparse matrix with shape: %s\" % str(X_train.shape)\n",
      "print"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cv.vocabulary_.keys()[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here, we create a variable, tfidf, which is a vectorizer responsible for performing three important steps:\n",
      "\n",
      "- First, it will build a dictionary of features where keys are terms and values are indices of the term in the feature matrix (that's the fit part in fit_transform)\n",
      "- Second, it will transform our documents into numerical feature vectors according to the frequency of words appearing in each text message. Since any one text message is short, each feature vector will be made up of mostly zeros, each of which indicates that a given word appeared zero times in that message.\n",
      "- Lastly, it will compute the tf-idf weights for our term frequency matrix.\n",
      "\n",
      "##Naive Bayes\n",
      "We'll be using SciKits' MultinomialNB, a Naive Bayes classifier effective for catching spam with the added benefits of scalability and low training time.\n",
      "\n",
      "**Why Multinomial Naive Bayes**?   \n",
      "Each row of the training set represents a document. A document is a list of $n$ words. If we consider that each word $w_i$ of the vocabulary appears in the vocabulary with probability $p_i$, then each document can be represented as a sample of a **multinomial distribution** with $n$ trials. The number of possible outcomes is the number of words in the vocabulary, and in each trial we choose a word from the vocabulary following the event probabilities $p_i$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#create a list of training labels. 1 is spam, 0 if ham\n",
      "y_train = y\n",
      " \n",
      "print \"y_train is a list of categories: %s ...\" % str(y_train)[:70]\n",
      "print \"X_train has %d feature vectors\" % (X_train.shape[0])\n",
      "print \"y_train has %d target classes\" %(len(y_train))\n",
      "print \"dataset has %d rows\" %(len(messages))\n",
      "print\n",
      " \n",
      "# create a Naive Bayes classifier\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "clf = MultinomialNB()\n",
      " \n",
      "clf.fit(X_train, y_train)\n",
      "print \"Trained MultinomialNB Classifier\"\n",
      "print \"Coefficients: %s ...\" % (str(clf.coef_)[:70])\n",
      "print \"   Intercept: %s\" %(str(clf.intercept_))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We create a variable, y_train, which is simply a list of target classes which our classifier will be trained to identify. 1 indicates spam while 0 indicates ham, or non-spam.\n",
      "\n",
      "Then we fit the model by passing the X_train sparse matrix and y_train to our MultinomialNB classifier's fit function.\n",
      "\n",
      "## Classifying New Observations\n",
      "\n",
      "Now let's classify the test documents as spam or not spam and see how we did."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_messages = [\"Call MobilesDirect free on 08000938767 to update now! or2stoptxt\",\n",
      "                 \"Call now for a free trial offer!\",\n",
      "                 \"Hey Sam, want to get some pizza later?\",\n",
      "                 \"idk my bff jill?\",\n",
      "                 \"Free later for a beer? Call me now!\"]\n",
      "                 \n",
      "# extract features from raw text documents\n",
      "C_test = cv.transform(test_messages)\n",
      "X_test = tfidf.transform(C_test)\n",
      " \n",
      "# MultinomialNB's predict classes directly\n",
      "print \"Classified: %s\" % clf.predict(X_test).astype(bool)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The predict function yields an array of True / False values (True for spam, False for not spam)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Model selection with cross validation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the previous example, we used the default value of parameter **alpha**, and the results were not very encouraging.\n",
      "Next, we searh for the best parameter **alpha** for the MultinomialNB model, and check the classification results for the test documents."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import grid_search\n",
      "\n",
      "gs = grid_search.GridSearchCV(clf, {'alpha': logspace(-2, 0, 20)}, cv=10, refit=True)\n",
      "gs.fit(X_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_messages = [\"Call MobilesDirect free on 08000938767 to update now! or2stoptxt\",\n",
      "                 \"Call now for a free trial offer!\",\n",
      "                 \"Hey Sam, want to get some pizza later?\",\n",
      "                 \"idk my bff jill?\",\n",
      "                 \"Free later for a beer? Call me now!\"]\n",
      "                 \n",
      "# extract features from raw text documents\n",
      "C_test = cv.transform(test_messages)\n",
      "X_test = tfidf.transform(C_test)\n",
      " \n",
      "# MultinomialNB's predict classes directly\n",
      "print \"Classified: %s\" % gs.predict(X_test).astype(bool)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}